{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f56fbd1",
   "metadata": {},
   "source": [
    "### Changepoint detection using CUSUM (NIST Method)\n",
    "\n",
    "#### Scenario 1: Sudden Drift\n",
    "\n",
    "##### Reference:\n",
    "NIST CUSUM example is at: https://www.itl.nist.gov/div898/handbook/pmc/section3/pmc323.htm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6561aa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm # Colormaps\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "np.random.seed(42)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc6ad43",
   "metadata": {},
   "source": [
    "The data for CUSUM-based changepoint detection is the AUC scores which are the outcome of the MLP classifier. The classifier was first tained using data samples from one distribution (in-control) which has a classificatoin AUC of 0.86. The model was then tested with the test data drawn from the same distribution. The model was also tested with data samples drawn from another distribution (out-of-control) with classification AUC 0.80. \n",
    "\n",
    "To simulate the sudden drift scenario, the trained model is first tested with the random samples drawn from the in-control distribution each day for 100 days (day0-99) followed by random samples drawn from the out-of-control distribution for the next 100 days (day 100-199).\n",
    "\n",
    "As a first step, load the data followed by train the classifier with the in-control data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93620863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data - AUC 0.86 and  AUC 0.80\n",
    "data86 = np.load('d0-AUC.86.npy')\n",
    "data80 = np.load('d5-AUC.80.npy')\n",
    "#print(data86.shape)\n",
    "#print(data80.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a4a78dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (16800, 2)\n",
      "Test (4200, 2)\n",
      "[[5910 1308]\n",
      " [ 881 8701]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.82      0.84      7218\n",
      "         1.0       0.87      0.91      0.89      9582\n",
      "\n",
      "    accuracy                           0.87     16800\n",
      "   macro avg       0.87      0.86      0.87     16800\n",
      "weighted avg       0.87      0.87      0.87     16800\n",
      "\n",
      "Specificity: 0.8187863674147964\n",
      "AUC: 0.863421570265528\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "#print(samples.shape)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('always') \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#--------------------------------------------------------\n",
    "# Train Classifier - MLP\n",
    "#--------------------------------------------------------\n",
    "samples = data86[:,[0,1]]\n",
    "labels = data86[:,2]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(samples, labels, test_size=0.20, random_state=5)\n",
    "print(\"Train\",X_train.shape)\n",
    "print(\"Test\",X_test.shape)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(2,4,4,1), activation='relu', solver='adam', max_iter=2000,learning_rate_init=0.001,learning_rate=\"constant\",random_state=4,shuffle=True,batch_size=8)\n",
    "mlp.fit(X_train,y_train)\n",
    "predict_train = mlp.predict(X_train)\n",
    "#predict_test = mlp.predict(X_test)\n",
    "\n",
    "#Evaluate the Model\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(confusion_matrix(y_train,predict_train))\n",
    "print(classification_report(y_train,predict_train))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_train,predict_train).ravel()\n",
    "specificity = tn / (tn+fp)\n",
    "\n",
    "AUC = roc_auc_score(y_train,predict_train)\n",
    "print(\"Specificity:\", specificity)\n",
    "print(\"AUC:\", AUC)\n",
    "#print(predict_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe6b1e6",
   "metadata": {},
   "source": [
    "Split the out-of-control data into train/test set and draw samples from the test set for simulating the sudden drift scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "671cf041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples from the second distribution -  (day100-199)\n",
    "new_samples = data80[:,[0,1]]\n",
    "new_labels = data80[:,2]\n",
    "\n",
    "X_train, X_test80, y_train, y_test80 = train_test_split(new_samples, new_labels, test_size=0.20, random_state=5)\n",
    "\n",
    "#X_test80.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36330428",
   "metadata": {},
   "source": [
    "Run 1000 simulations of day0-100 followed by day 100-199, save the AUCs recorded for changepoint detection using CUSUM algorithm. \n",
    "\n",
    "Evaluate:\n",
    "#FPs, #TPs, average, minimum and maximum delay to detect the change point. Each experiment either has a TP or an FP. Once a TP or FP is detected, break out of the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72f77b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate 1000 days - Samples from AUC(0.86) from day 0-99 and AUC(0.80) from day100-199\n",
    "runs = 0\n",
    "FalsePos =  np.array([])\n",
    "TruePos  =  np.array([])\n",
    "timetoDetect =  np.array([])\n",
    "while (runs < 1000):\n",
    "    test_days = 0\n",
    "    test_AUC =  np.array([])\n",
    "    while (test_days < 100):     #day0-99 from AUC(0.86)\n",
    "        test_samples = np.array([])\n",
    "        test_labels = np.array([])\n",
    "    \n",
    "\n",
    "        number_of_rows = X_test.shape[0]\n",
    "        random_indices = np.random.choice(number_of_rows, \n",
    "                                  size=50, \n",
    "                                  replace=False)\n",
    "    \n",
    "    \n",
    "        test_samples = X_test[random_indices,:]\n",
    "        test_labels  = y_test[random_indices]\n",
    "        #test_samples.shape\n",
    "        #print(y_test.shape)\n",
    "        #test_labels\n",
    "\n",
    "        predict_test = mlp.predict(test_samples)\n",
    "\n",
    "        #print(confusion_matrix(test_labels,predict_test))\n",
    "        #print(classification_report(test_labels,predict_test))\n",
    "        tn, fp, fn, tp = confusion_matrix(test_labels,predict_test).ravel()\n",
    "        specificity = tn / (tn+fp)\n",
    "\n",
    "\n",
    "        AUC = roc_auc_score(test_labels,predict_test)\n",
    "        #print(\"Specificity:\", specificity)\n",
    "        #print(\"AUC:\", AUC)\n",
    "\n",
    "        test_AUC = np.append(test_AUC, AUC)\n",
    "        #print(\"day\",test_days)\n",
    "        #AUC\n",
    "        test_days += 1\n",
    "    \n",
    "    while (test_days < 200):    #day100-199 from AUC(0.80)\n",
    "        test_samples80 = np.array([])\n",
    "        test_labels80 = np.array([])\n",
    "    \n",
    "\n",
    "        number_of_rows80 = X_test80.shape[0]\n",
    "        random_indices80 = np.random.choice(number_of_rows80, \n",
    "                                  size=50, \n",
    "                                  replace=False)\n",
    "    \n",
    "    \n",
    "        test_samples80 = X_test80[random_indices80,:]\n",
    "        test_labels80  = y_test80[random_indices80]\n",
    "        #test_samples.shape\n",
    "        #print(y_test.shape)\n",
    "        #test_labels\n",
    "\n",
    "        predict_test80 = mlp.predict(test_samples80)\n",
    "\n",
    "        #print(confusion_matrix(test_labels,predict_test))\n",
    "        #print(classification_report(test_labels,predict_test))\n",
    "        tn80, fp80, fn80, tp80 = confusion_matrix(test_labels80,predict_test80).ravel()\n",
    "        specificity80 = tn80 / (tn80+fp80)\n",
    "\n",
    "\n",
    "        AUC80 = roc_auc_score(test_labels80,predict_test80)\n",
    "        #print(\"Specificity:\", specificity)\n",
    "        #print(\"AUC:\", AUC)\n",
    "\n",
    "        test_AUC = np.append(test_AUC, AUC80)\n",
    "        #print(\"day\",test_days)\n",
    "        #AUC\n",
    "        test_days += 1\n",
    "    \n",
    "    #CUSUM for day0-200: outcomes are delay and #FP, #FP+#TP\n",
    "    num_rows = np.shape(test_AUC)[0]\n",
    "    in_control_auc = test_AUC[:100]\n",
    "    x = np.array(test_AUC)\n",
    "\n",
    "    mu   = np.mean(in_control_auc)\n",
    "    std  = np.std(in_control_auc)\n",
    "    h    = 0.25       # A difference of 0.5 AUC will be considered out of control - Similar to threshold from the detecta package\n",
    "    k    = 0.05    # Similar to drift in the quantile calculation - 0.05 is the 1 sigma change we wish to detect\n",
    "    \n",
    "    x_mean = np.zeros(num_rows,dtype=float)\n",
    "    #S_hi : for positive changes --------------------------\n",
    "    S_hi = np.zeros(num_rows,dtype=float)\n",
    "    S_hi[0] = 0.0 # starts with 0\n",
    "    #Increase in mean = x-mu-k ----------------------------\n",
    "    mean_hi = np.zeros(num_rows,dtype=float)\n",
    "\n",
    "    #Decrease in mean = mu-k-x----------------------------\n",
    "    mean_lo = np.zeros(num_rows,dtype=float)\n",
    "    #S_lo : for negative changes --------------------------\n",
    "    S_lo = np.zeros(num_rows,dtype=float)\n",
    "    S_lo[0] = 0.0 # starts with 0\n",
    "     #CUSUM: Cumulative sum of x minus mu ------------------\n",
    "    cusum = np.zeros(num_rows,dtype=float)\n",
    "    cusum[0] = 0.0 # initialize with 0\n",
    "    \n",
    "    for i in range(0, num_rows):\n",
    "        x_mean[i]  = x[i] - mu  #x_mean \n",
    "        mean_hi[i] = x[i] - mu - k\n",
    "        S_hi[i]    = max(0, S_hi[i-1] + mean_hi[i])\n",
    "        mean_lo[i] = mu - k - x[i]\n",
    "        S_lo[i]    = max(0, S_lo[i-1] + mean_lo[i])\n",
    "        cusum[i]   = cusum[i-1] + x_mean[i]\n",
    "\n",
    "    x_mean  = np.round(x_mean,decimals=2)\n",
    "    S_hi    = np.round(S_hi,decimals=2)\n",
    "    mean_lo = np.round(mean_lo,decimals=2)\n",
    "    S_lo    = np.round(S_lo,decimals=2)\n",
    "    cusum   = np.round(cusum,decimals=2)\n",
    "\n",
    "    # Construct the tabular CUSUM Chart\n",
    "    chart = np.array([])\n",
    "    chart = np.column_stack((x.T, x_mean.T, mean_hi.T, S_hi.T, mean_lo.T, S_lo.T, cusum.T))\n",
    "    np.round(chart, 2)\n",
    "\n",
    "    #d = 2 *(np.log((1-0.01) / (0.0027)))\n",
    "    #h = d * 0.5 # h= d*k where k=0.5\n",
    "    #h = 4 # as per the NIST doc on CUSUM\n",
    "\n",
    "    #l1 =  np.append(num_rows, data_tabular, axis = 1)\n",
    "    #l1 = np.concatenate(num_rows.T, data_tabular.T)\n",
    "    #chart = np.column_stack((num_rows.T, data_tabular.T))\n",
    "    #chart\n",
    "\n",
    "    np.set_printoptions(suppress=True, formatter={'float_kind':'{:0.2f}'.format})\n",
    "    #print(\"CUSUM Chart is:\\n\", np.round(chart,decimals=2))\n",
    "    #x_mean\n",
    "\n",
    "    df = pd.DataFrame(chart) \n",
    "    df.columns = ['X','x-mu','Increase in Mean', 'S_hi', 'Decrease-in-mean', 'S_lo', 'CUSUM']\n",
    "\n",
    "    #print(df.to_string())\n",
    "    #print(chart)\n",
    "    \n",
    "    # False positives and Total alarms\n",
    "    falsePos = 0\n",
    "    alarms   = 0\n",
    "    delay = 0\n",
    "    for i in range(0, num_rows):\n",
    "        if (S_lo[i] > h):   \n",
    "            if (i>100):\n",
    "                alarms += 1        #TP\n",
    "                delay   = i-100+1  # ts is 100 because the change starts at day100\n",
    "            #break\n",
    "            #print(S_lo[i])\n",
    "        if (S_lo[i] > h): \n",
    "            if (i<100):\n",
    "                falsePos += 1  #FP \n",
    "            break\n",
    "\n",
    "    # Delay to detect the first changepoint\n",
    "    #delay = 0\n",
    "    #for i in range(0, num_rows):\n",
    "    #    if (S_lo[i] > h):\n",
    "    #        delay = i-100+1 # ts is 100 because the change starts at day100\n",
    "    #        break\n",
    "    \n",
    "    FalsePos = np.append(FalsePos, falsePos)\n",
    "    TruePos  = np.append(TruePos, alarms)\n",
    "    timetoDetect = np.append(timetoDetect, delay)\n",
    "    #print(falsePos)\n",
    "    runs += 1  # continue until 1000 runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "934809fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of FPs 3.0\n",
      "TPs 997.0\n",
      "avg delay to detect 11.115\n",
      "min Delay 0.0\n",
      "max Delay 62.0\n"
     ]
    }
   ],
   "source": [
    "#test_AUC.shape\n",
    "#num_rows = np.shape(test_AUC)[0]\n",
    "#num_rows\n",
    "#in_control_auc = test_AUC[:100]\n",
    "#in_control_auc.shape\n",
    "#x = np.array(test_AUC)\n",
    "#test_AUC.shape\n",
    "#x.shape\n",
    "print(\"total number of FPs\",np.sum(FalsePos))\n",
    "print(\"TPs\",np.sum(TruePos))\n",
    "print(\"avg delay to detect\",np.mean(timetoDetect))\n",
    "print(\"min Delay\",np.min(timetoDetect))\n",
    "print(\"max Delay\",np.max(timetoDetect))\n",
    "#timetoDetect\n",
    "#FalsePos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "597f7373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.00, 5.00, 7.00, 6.00, 3.00, 0.00, 6.00, 4.00, 4.00, 0.00, 4.00,\n",
       "       3.00, 5.00, 3.00, 3.00, 7.00, 5.00, 0.00, 4.00, 7.00, 4.00, 3.00,\n",
       "       4.00, 3.00, 7.00, 0.00, 5.00, 5.00, 6.00, 4.00, 5.00, 0.00, 2.00,\n",
       "       6.00, 3.00, 5.00, 2.00, 4.00, 2.00, 3.00, 6.00, 3.00, 8.00, 7.00,\n",
       "       4.00, 0.00, 3.00, 5.00, 9.00, 6.00, 4.00, 7.00, 0.00, 0.00, 4.00,\n",
       "       0.00, 9.00, 6.00, 4.00, 2.00, 2.00, 6.00, 10.00, 5.00, 11.00, 8.00,\n",
       "       2.00, 6.00, 12.00, 4.00, 4.00, 0.00, 4.00, 3.00, 2.00, 4.00, 4.00,\n",
       "       2.00, 3.00, 0.00, 7.00, 3.00, 0.00, 6.00, 4.00, 4.00, 5.00, 7.00,\n",
       "       3.00, 11.00, 8.00, 4.00, 8.00, 4.00, 2.00, 16.00, 0.00, 8.00, 5.00,\n",
       "       5.00, 3.00, 0.00, 0.00, 19.00, 6.00, 2.00, 0.00, 8.00, 3.00, 0.00,\n",
       "       9.00, 8.00, 0.00, 3.00, 7.00, 3.00, 6.00, 5.00, 3.00, 10.00, 2.00,\n",
       "       4.00, 0.00, 4.00, 0.00, 2.00, 5.00, 0.00, 4.00, 2.00, 3.00, 2.00,\n",
       "       18.00, 2.00, 12.00, 8.00, 6.00, 7.00, 6.00, 4.00, 5.00, 4.00, 3.00,\n",
       "       4.00, 5.00, 5.00, 7.00, 10.00, 4.00, 2.00, 2.00, 17.00, 5.00, 8.00,\n",
       "       3.00, 9.00, 9.00, 14.00, 3.00, 7.00, 2.00, 2.00, 8.00, 5.00, 0.00,\n",
       "       3.00, 4.00, 4.00, 5.00, 12.00, 3.00, 6.00, 4.00, 4.00, 17.00, 5.00,\n",
       "       5.00, 3.00, 3.00, 14.00, 2.00, 5.00, 5.00, 0.00, 6.00, 4.00, 3.00,\n",
       "       10.00, 0.00, 5.00, 3.00, 0.00, 6.00, 4.00, 5.00, 0.00, 7.00, 4.00,\n",
       "       0.00, 0.00, 7.00, 5.00, 5.00, 5.00, 5.00, 2.00, 0.00, 3.00, 6.00,\n",
       "       13.00, 3.00, 10.00, 3.00, 3.00, 11.00, 2.00, 2.00, 2.00, 4.00,\n",
       "       0.00, 4.00, 9.00, 3.00, 7.00, 6.00, 8.00, 0.00, 0.00, 6.00, 3.00,\n",
       "       0.00, 7.00, 8.00, 8.00, 6.00, 4.00, 0.00, 5.00, 6.00, 9.00, 3.00,\n",
       "       6.00, 9.00, 6.00, 2.00, 7.00, 0.00, 3.00, 6.00, 7.00, 6.00, 6.00,\n",
       "       3.00, 5.00, 7.00, 16.00, 11.00, 5.00, 6.00, 5.00, 8.00, 3.00,\n",
       "       14.00, 3.00, 5.00, 0.00, 6.00, 3.00, 5.00, 0.00, 3.00, 4.00, 7.00,\n",
       "       0.00, 5.00, 0.00, 0.00, 6.00, 0.00, 4.00, 7.00, 4.00, 11.00, 3.00,\n",
       "       6.00, 8.00, 5.00, 6.00, 4.00, 8.00, 10.00, 4.00, 0.00, 7.00, 8.00,\n",
       "       4.00, 0.00, 3.00, 7.00, 13.00, 2.00, 5.00, 8.00, 9.00, 7.00, 5.00,\n",
       "       5.00, 2.00, 4.00, 13.00, 4.00, 3.00, 3.00, 9.00, 3.00, 6.00, 9.00,\n",
       "       2.00, 0.00, 3.00, 8.00, 3.00, 7.00, 16.00, 2.00, 5.00, 0.00, 4.00,\n",
       "       0.00, 0.00, 2.00, 6.00, 4.00, 4.00, 0.00, 4.00, 6.00, 2.00, 10.00,\n",
       "       5.00, 12.00, 2.00, 4.00, 3.00, 8.00, 2.00, 5.00, 12.00, 5.00, 3.00,\n",
       "       3.00, 0.00, 6.00, 3.00, 8.00, 2.00, 0.00, 2.00, 0.00, 2.00, 0.00,\n",
       "       2.00, 0.00, 6.00, 3.00, 4.00, 2.00, 6.00, 4.00, 10.00, 6.00, 4.00,\n",
       "       8.00, 6.00, 15.00, 0.00, 0.00, 6.00, 5.00, 2.00, 4.00, 7.00, 8.00,\n",
       "       0.00, 0.00, 2.00, 0.00, 3.00, 0.00, 3.00, 3.00, 5.00, 4.00, 7.00,\n",
       "       4.00, 4.00, 0.00, 3.00, 6.00, 0.00, 0.00, 6.00, 4.00, 7.00, 4.00,\n",
       "       4.00, 3.00, 2.00, 11.00, 11.00, 7.00, 6.00, 6.00, 3.00, 8.00, 3.00,\n",
       "       2.00, 0.00, 6.00, 3.00, 0.00, 2.00, 0.00, 2.00, 11.00, 11.00, 9.00,\n",
       "       4.00, 5.00, 0.00, 0.00, 0.00, 8.00, 13.00, 8.00, 5.00, 0.00, 4.00,\n",
       "       4.00, 13.00, 3.00, 5.00, 4.00, 0.00, 7.00, 0.00, 4.00, 4.00, 3.00,\n",
       "       17.00, 4.00, 0.00, 5.00, 0.00, 0.00, 5.00, 5.00, 2.00, 2.00, 2.00,\n",
       "       0.00, 2.00, 0.00, 2.00, 0.00, 2.00, 4.00, 8.00, 0.00, 5.00, 7.00,\n",
       "       5.00, 0.00, 3.00, 5.00, 2.00, 3.00, 8.00, 4.00, 2.00, 6.00, 5.00,\n",
       "       3.00, 5.00, 0.00, 7.00, 4.00, 7.00, 8.00, 2.00, 11.00, 0.00, 3.00,\n",
       "       4.00, 2.00, 11.00, 7.00, 5.00, 0.00, 8.00, 8.00, 0.00, 6.00, 7.00,\n",
       "       0.00, 5.00, 2.00, 5.00, 4.00, 0.00, 4.00, 9.00, 4.00, 5.00, 0.00,\n",
       "       7.00, 4.00, 6.00, 5.00, 4.00, 0.00, 0.00, 8.00, 4.00, 3.00, 6.00,\n",
       "       2.00, 0.00, 0.00, 4.00, 2.00, 4.00, 7.00, 8.00, 5.00, 6.00, 2.00,\n",
       "       10.00, 10.00, 7.00, 4.00, 8.00, 5.00, 6.00, 7.00, 3.00, 0.00, 0.00,\n",
       "       3.00, 0.00, 4.00, 3.00, 4.00, 2.00, 9.00, 6.00, 2.00, 7.00, 4.00,\n",
       "       4.00, 5.00, 6.00, 8.00, 3.00, 8.00, 4.00, 6.00, 4.00, 6.00, 0.00,\n",
       "       8.00, 5.00, 5.00, 5.00, 3.00, 7.00, 8.00, 0.00, 4.00, 3.00, 5.00,\n",
       "       15.00, 10.00, 2.00, 5.00, 5.00, 0.00, 3.00, 4.00, 5.00, 3.00, 5.00,\n",
       "       0.00, 3.00, 2.00, 2.00, 3.00, 3.00, 14.00, 6.00, 2.00, 0.00, 10.00,\n",
       "       8.00, 6.00, 2.00, 0.00, 5.00, 3.00, 3.00, 2.00, 5.00, 9.00, 5.00,\n",
       "       3.00, 5.00, 14.00, 4.00, 4.00, 2.00, 6.00, 3.00, 4.00, 9.00, 0.00,\n",
       "       2.00, 13.00, 5.00, 11.00, 0.00, 3.00, 4.00, 8.00, 6.00, 4.00, 7.00,\n",
       "       4.00, 5.00, 7.00, 7.00, 4.00, 0.00, 7.00, 14.00, 10.00, 4.00, 7.00,\n",
       "       2.00, 2.00, 12.00, 0.00, 5.00, 0.00, 0.00, 3.00, 4.00, 2.00, 2.00,\n",
       "       2.00, 3.00, 7.00, 5.00, 5.00, 4.00, 3.00, 5.00, 5.00, 0.00, 2.00,\n",
       "       7.00, 4.00, 7.00, 6.00, 3.00, 0.00, 3.00, 3.00, 5.00, 8.00, 3.00,\n",
       "       2.00, 10.00, 4.00, 3.00, 18.00, 0.00, 0.00, 3.00, 5.00, 5.00, 0.00,\n",
       "       5.00, 5.00, 7.00, 0.00, 0.00, 0.00, 5.00, 0.00, 5.00, 6.00, 6.00,\n",
       "       7.00, 7.00, 6.00, 5.00, 4.00, 0.00, 3.00, 9.00, 4.00, 4.00, 8.00,\n",
       "       13.00, 6.00, 5.00, 11.00, 2.00, 3.00, 5.00, 0.00, 0.00, 0.00, 3.00,\n",
       "       15.00, 0.00, 0.00, 0.00, 4.00, 0.00, 6.00, 2.00, 5.00, 2.00, 0.00,\n",
       "       4.00, 7.00, 3.00, 0.00, 0.00, 3.00, 3.00, 11.00, 11.00, 0.00, 6.00,\n",
       "       6.00, 0.00, 0.00, 4.00, 11.00, 3.00, 4.00, 5.00, 0.00, 3.00, 11.00,\n",
       "       4.00, 4.00, 6.00, 0.00, 3.00, 5.00, 2.00, 4.00, 0.00, 2.00, 0.00,\n",
       "       8.00, 2.00, 5.00, 6.00, 7.00, 6.00, 3.00, 2.00, 16.00, 0.00, 7.00,\n",
       "       8.00, 3.00, 17.00, 0.00, 5.00, 6.00, 5.00, 5.00, 4.00, 10.00, 2.00,\n",
       "       4.00, 4.00, 0.00, 11.00, 6.00, 7.00, 14.00, 3.00, 10.00, 2.00,\n",
       "       4.00, 0.00, 0.00, 3.00, 5.00, 4.00, 3.00, 4.00, 0.00, 3.00, 3.00,\n",
       "       2.00, 5.00, 0.00, 4.00, 6.00, 0.00, 0.00, 0.00, 0.00, 6.00, 3.00,\n",
       "       8.00, 4.00, 5.00, 4.00, 4.00, 5.00, 3.00, 4.00, 2.00, 2.00, 8.00,\n",
       "       2.00, 4.00, 4.00, 4.00, 0.00, 7.00, 10.00, 12.00, 2.00, 4.00, 0.00,\n",
       "       4.00, 9.00, 0.00, 0.00, 6.00, 8.00, 4.00, 6.00, 8.00, 3.00, 4.00,\n",
       "       5.00, 6.00, 4.00, 4.00, 0.00, 4.00, 13.00, 6.00, 13.00, 5.00, 4.00,\n",
       "       5.00, 6.00, 4.00, 2.00, 0.00, 3.00, 0.00, 0.00, 3.00, 2.00, 2.00,\n",
       "       6.00, 3.00, 0.00, 2.00, 7.00, 3.00, 3.00, 8.00, 3.00, 7.00, 9.00,\n",
       "       3.00, 0.00, 4.00, 6.00, 10.00, 5.00, 5.00, 2.00, 3.00, 0.00, 5.00,\n",
       "       2.00, 4.00, 11.00, 4.00, 5.00, 7.00, 8.00, 0.00, 4.00, 11.00,\n",
       "       10.00, 0.00, 5.00, 3.00, 0.00, 0.00, 3.00, 3.00, 6.00, 4.00, 7.00,\n",
       "       3.00, 4.00, 0.00, 3.00, 4.00, 3.00, 5.00, 2.00, 8.00, 0.00, 6.00,\n",
       "       10.00, 0.00, 2.00, 6.00, 4.00, 5.00, 2.00, 3.00, 4.00, 0.00, 2.00,\n",
       "       4.00, 11.00, 0.00, 0.00, 6.00, 0.00, 11.00, 2.00, 8.00, 3.00, 2.00,\n",
       "       4.00, 5.00, 4.00, 2.00, 4.00, 10.00, 5.00, 0.00, 0.00, 2.00, 2.00,\n",
       "       6.00, 0.00, 2.00, 3.00, 4.00, 7.00, 3.00, 5.00, 5.00, 9.00, 8.00,\n",
       "       0.00, 3.00, 2.00, 7.00, 9.00, 0.00, 4.00, 0.00, 6.00, 6.00, 2.00,\n",
       "       9.00, 8.00, 4.00, 0.00, 3.00, 0.00, 5.00, 5.00, 3.00, 2.00, 11.00,\n",
       "       5.00, 4.00, 3.00])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timetoDetect\n",
    "#FalsePos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2daac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CUSUM\n",
    "plt.plot(cusum, '-b')\n",
    "plt.title('Cumulative Sum')\n",
    "plt.show()\n",
    "\n",
    "# specifying horizontal line type\n",
    "plt.axhline(y = 0.25, color = 'b', linestyle = '--')\n",
    "plt.plot(S_hi,  '-g', label='positive changes')\n",
    "plt.plot(S_lo, '-r', label='negative changes')\n",
    "plt.title('Positive and Negative Changes')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Threshold vs. delay\n",
    "# 1st line\n",
    "point_1 = [0.5,12.2]\n",
    "point_2 = [1,24]\n",
    "point_3 = [0.2,-2.103]\n",
    "\n",
    "# 2nd line\n",
    "point_4 = [0.2,8]\n",
    "point_5 = [0.5,22.8]\n",
    "point_6 = [0.25,11.7]\n",
    "\n",
    "x_values = [[point_1[0], point_3[0]],[point_2[0], point_4[0]]]\n",
    "y_values = [[point_1[1], point_3[1]],[point_2[1], point_4[1]]]\n",
    "\n",
    "plt.plot(x_values, y_values, 'red')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
